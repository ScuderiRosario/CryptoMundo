version: '3'
x-airflow-common:
  &airflow-common
  build:
      context: .
      dockerfile: dockerfile
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
    - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__CORE__LOGGING_LEVEL=INFO
    - _PIP_ADDITIONAL_REQUIREMENTS=pycoingecko snscrape vaderSentiment kafka-python pyspark findspark yfinance
    - POSTGRES_PASSWORD=airflow 

  volumes:
    - ./dags:/opt/airflow/dags
    - ./airflow-data/logs:/opt/airflow/logs
    - ./airflow-data/plugins:/opt/airflow/plugins
    - ./airflow-data/airflow.cfg:/opt/airlfow/airflow.cfg
    - ./airflow-data/data:/opt/airlfow/data

  depends_on:
    - postgres

services:
  postgres:
    image: postgres:latest
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=airflow
      - POSTGRES_PORT=5432
    volumes:
      - ./postgres_db_volume:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  airflow-init:
    << : *airflow-common
    container_name: airflow_init
    entrypoint: /bin/bash
    command:
      - -c
      - airflow users list || ( airflow db init &&
        airflow users create
          --role Admin
          --username airflow
          --password airflow
          --email airflow@airflow.com
          --firstname airflow
          --lastname airflow )
    restart: on-failure

  airflow-webserver:
    << : *airflow-common
    user: airflow
    command: airflow webserver -p 9999 
    ports:
      - "9999:9999"
    container_name: airflow_webserver
    restart: always      

  airflow-scheduler:
    << : *airflow-common
    command: airflow scheduler
    container_name: airflow_scheduler
    restart: always

  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka:6.2.0
    container_name: broker
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT,
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://broker:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
  
  elasticsearch:
        image: docker.elastic.co/elasticsearch/elasticsearch:7.5.2
        restart: always
        depends_on: 
            -  airflow-webserver
        environment:
            ES_JAVA_OPTS: "-Xmx256m -Xms256m"
            discovery.type: single-node
        ports: 
            - "9200:9200"
            - "9300:9300"

  kibana:
        image: docker.elastic.co/kibana/kibana:7.5.2
        restart: always
        depends_on:
            - elasticsearch
        environment: 
            ELASTICSEARCH_HOSTS: http://elasticsearch:9200
            SERVER_HOST: "0.0.0.0"
        ports: 
            - "5601:5601"

  logstash:
        image: docker.elastic.co/logstash/logstash:7.5.2
        restart: always
        depends_on:
            - elasticsearch
        volumes:
            - ./logstash:/usr/share/logstash/pipeline

  spark-master:
    image: bde2020/spark-master:3.2.0-hadoop3.2
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark

  spark-worker-1:
    image: bde2020/spark-worker:3.2.0-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"

  spark-worker-2:
    image: bde2020/spark-worker:3.2.0-hadoop3.2
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"

    #Jupyter notebook
  jupyter-spark:
        image: jupyter/pyspark-notebook:spark-3.1.2
        ports:
          - "8888:8888"
          - "4040-4080:4040-4080"
        volumes:
          - ./jupyter:/home/jovyan/work/notebooks/
          - ./spark/resources/data:/home/jovyan/work/data/
          - ./spark/resources/jars:/home/jovyan/work/jars/

networks:
    default_net: